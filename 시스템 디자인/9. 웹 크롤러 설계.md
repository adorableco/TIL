
### 웹 크롤러란?

- 로봇(robot) 또는 스파이더(spider) 라고도 부름
- 검색 엔진에서 널리 쓰는 기술

#### 사용 분야
- 검색 엔진 인덱싱 (search engine indexing)
	- 크롤러는 웹 페이지를 모아 검색 엔진을 위한 local index를 만듦
- 웹 아카이빙 (web archiving)
	- 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차
- 웹 마이닝 (web mining)
	- 인터넷에서 유용한 지식을 도출해냄
- 웹 모니터링
	- 인터넷에서 저작권이나 상표권이 침해되는 사례 모니터링 가능


<img src="https://github.com/user-attachments/assets/6a0e6729-4aa1-4821-b1c1-d897cc468600" width="500">


 - **시작 URL 집합**
 - **미수집 URL 저장소**
	 - '다운로드할 URL' 과 '다운로드된 URL' 중 '다운로드할 URL' 을 저장 관리하는 컴포넌트
	 - FIFO 큐
- **HTML 다운로더**
	- 인터넷에서 웹 페이지를 다운로드하는 컴포넌트
- **도메인 이름 변환기**
	- 웹 페이지를 다운받으려면 URL을 IP 주소로 변환하는 절차가 필요함
- **콘텐츠 파서**
	- 이상한 웹 페이지를 걸러내기 위한 컴포넌트
- **중복 콘텐츠인가?**
	- 29% 가량의 웹페이지 콘텐츠는 중복임
	- 웹 페이지의 해시 값 비교 또는 두 HTML 문서의 문자열을 비교하는 방법이 있음
		- 전자가 효과적
	- **이미 저장소에 있는 콘텐츠인 경우**: 처리하지 않고 버림
	- **저장소에 없는 콘텐츠인 경우**: 저장소에 저장한 뒤 URL 추출기로 전달
- **콘텐츠 저장소**
	- HTML 문서를 보관하는 시스템
- **URL 추출기**
	- HTML 페이지를 파싱하여 링크들을 골라내는 역할을 함
	- 페이지 내의 링크들을 또 발견해나가는 과정
- **URL 필터**
	- deny list에 포함된 URL, 특정 콘텐츠 타입이나 파일 확장자를 갖는 URL을 크롤링 대상에서 배제하는 역할
- **이미 방문한 URL?**
	- 이미 방문한 적이 있는 URL인지 추적하여 같은 URL을 여러번 처리하지 않도록 함
	- bloom filter나 해시테이블 자료구조를 사용함
- **URL 저장소**
	- 이미 방문한 URL을 보관하는 저장소


### DFS vs. BFS

- 웹이 어느 정도로 깊은지 가늠하기가 어려우므로 웹 크롤러는 보통 BFS (너비 우선 탐색법) 을 사용함

#### BFS 문제점
- 한 페이지에서 나오는 링크의  상당수는 같은 서버로 되돌아감
- 크롤러는 같은 호스트에 속한 많은 링크를 다운받게 됨
	- ➡️ 이 링크들을 병렬로 처리하게 되면 **'예의 없는(impolite)' 크롤러**로 간주됨

### 미수집 URL 저장소

- '호스트명'과 '다운로드를 수행하는 작업 스레드 사이의 맵'을 구성해서 같은 웹 사이트의 페이지를 다운받는 태스크는 시간차를 두고 실행하도록 함

| 호스트           | 큐   |
| ------------- | --- |
| wikipedia.com | b1  |
| apple.com     | b2  |
| ...           | ... |
| naver.com     | bn  |

<img src="https://github.com/user-attachments/assets/12cd47e5-7efe-4687-b7c0-72a69b537288" width="500">

- `queue router` : 같은 호스트에 속한 URL이 언제나 같은 큐로 가도록 보장하는 역할
- `mapping table`: 호스트 이름과 큐 사이의 관계를 보관하는 테이블
- `FIFO 큐`: 같은 호스트에 속한 URL은 언제나 같은 큐에 보관
- `queue selector`: 큐들을 순회하면서 큐에서 URL을 꺼내서 작업 스레드에 전달
- `작업 스레드` : 전달된 URL을 다운로드하는 작업을 수행


#### 우선순위

- 우선순위를 고려한 설계


<img src="https://github.com/user-attachments/assets/fd8271d4-6be2-4988-afc4-6b844791e053" width="500">


#### 신선도

- 의 신선함을 유지하기 위해서는 이미 다운로드한 페이지라고 해도 주기적으로 재수집해야 함

### HTML 다운로더

#### Robots.txt
- 로봇 제외 프로토콜 (Robot Exclusion Protocol)
- 이 파일에는 크롤러가 수집해도 되는 페이지 목록이 들어있음

#### 성능 최적화

1. 분산 크롤링
2. 도메인 이름 변환 결과 캐시
- DNS 리졸버는 크롤러 성능의 병목 중 하나임 ➡️ DNS 요청 결과를 받기 전까지는 다음 작업을 진행할 수없는 동기적 특성 때문 
3. 지역성
- 크롤링 작업을 수행하는 서버를 지역별로 분산하는 방법
4. 짧은 타임아웃



***
### 마무리 

- 서버 사이드 렌더링인 경우
	- 페이지를 파싱하기 전에 서버 사이드 렌더링을 적용하여 해결해야 함
	- 웹 페이지를 그냥 있는 그대로 다운받아서 파싱하면 동적으로 생성되는 링크는 발견할 수 없기 때문